{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f795fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667e1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf349bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'MAG' \n",
    "sub_dataset='Mathematics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baddf92",
   "metadata": {},
   "source": [
    "# Generate Pretraining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba7e9e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 490551/490551 [00:51<00:00, 9545.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# read raw data\n",
    "with open(f'data_dir/MAG/{sub_dataset}/papers_bert.json') as f:\n",
    "    data = {}\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        #data.append(json.loads(line))\n",
    "        #data.append(eval(line.strip()))\n",
    "        tmp = eval(line.strip())\n",
    "        data[tmp['paper']] = tmp\n",
    "#random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c84fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14271/14271 [00:00<00:00, 805773.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique labels:14010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read label name dict\n",
    "label_name_dict = {}\n",
    "label_name_set = set()\n",
    "label_name2id_dict = {}\n",
    "\n",
    "with open(f'data_dir/MAG/{sub_dataset}/labels.txt') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = line.strip().split('\\t')\n",
    "        label_name_dict[tmp[0]] = tmp[1]\n",
    "        label_name2id_dict[tmp[1]] = tmp[0]\n",
    "        label_name_set.add(tmp[1])\n",
    "\n",
    "print(f'Num of unique labels:{len(label_name_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d00a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 490551/490551 [00:03<00:00, 147930.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# filter related\n",
    "\n",
    "idd_set = set(list(data.keys()))\n",
    "\n",
    "for idd in tqdm(data):\n",
    "    if 'reference' not in data[idd] or len(data[idd]['reference']) == 0:\n",
    "        continue\n",
    "        \n",
    "    data[idd]['reference'] = list(set(data[idd]['reference']) & idd_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f8fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text processing function\n",
    "def text_process(text):\n",
    "    p_text = ' '.join(text.split('\\r\\n'))\n",
    "    p_text = ' '.join(p_text.split('\\n\\r'))\n",
    "    p_text = ' '.join(p_text.split('\\n'))\n",
    "    p_text = ' '.join(p_text.split('\\t'))\n",
    "    p_text = ' '.join(p_text.split('\\rm'))\n",
    "    p_text = ' '.join(p_text.split('\\r'))\n",
    "    p_text = ''.join(p_text.split('$'))\n",
    "    p_text = ''.join(p_text.split('*'))\n",
    "\n",
    "    return p_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e606b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 490551/490551 [00:00<00:00, 754943.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg ref cnt:6.276238238575368.\n",
      "ref papers:380056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average edge\n",
    "\n",
    "ref_cnt = 0\n",
    "ref_paper = {}\n",
    "\n",
    "\n",
    "for idd in tqdm(data):\n",
    "    if 'reference' not in data[idd] or len(data[idd]['reference']) == 0:\n",
    "        continue\n",
    "        \n",
    "    ref_cnt += len(data[idd]['reference'])\n",
    "    ref_paper[idd] = data[idd]\n",
    "\n",
    "print(f'avg ref cnt:{ref_cnt/len(ref_paper)}.')\n",
    "print(f'ref papers:{len(ref_paper)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "691d2a4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 380056/380056 [00:10<00:00, 36588.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test size:1733292,194157,455459\n",
      "Train/Val/Test avg:4.560622645083883,0.5108641884353885,1.1983997095164922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## split train/val/test as 8:1:1\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "test_pairs = []\n",
    "train_pair_set = set()\n",
    "item_id2idx = {}\n",
    "train_neighbor = defaultdict(list)\n",
    "val_neighbor = defaultdict(list)\n",
    "test_neighbor = defaultdict(list)\n",
    "\n",
    "for iid in tqdm(ref_paper):\n",
    "    if iid not in item_id2idx:\n",
    "        item_id2idx[iid] = len(item_id2idx)\n",
    "    \n",
    "    also_viewed = ref_paper[iid]['reference']\n",
    "    random.shuffle(also_viewed)\n",
    "    \n",
    "    for i in range(int(len(also_viewed)*0.8)):\n",
    "        train_pairs.append((iid,also_viewed[i]))\n",
    "        train_pair_set.add((iid,also_viewed[i]))\n",
    "        train_pair_set.add((also_viewed[i],iid))\n",
    "        \n",
    "        # add to item_id2idx\n",
    "        if also_viewed[i] not in item_id2idx:\n",
    "            item_id2idx[also_viewed[i]] = len(item_id2idx)\n",
    "\n",
    "        # add to train_user_neighbor/train_item_neighbor\n",
    "        train_neighbor[iid].append(also_viewed[i])\n",
    "\n",
    "    for i in range(int(len(also_viewed)*0.8),int(len(also_viewed)*0.9)):\n",
    "        if (iid,also_viewed[i]) in train_pair_set:\n",
    "            continue\n",
    "        val_pairs.append((iid,also_viewed[i]))\n",
    "        assert (iid,also_viewed[i]) not in train_pair_set\n",
    "\n",
    "        # add to item_id2idx\n",
    "        if also_viewed[i] not in item_id2idx:\n",
    "            item_id2idx[also_viewed[i]] = len(item_id2idx)\n",
    "        \n",
    "        # add to train_user_neighbor/train_item_neighbor\n",
    "        val_neighbor[iid].append(also_viewed[i])\n",
    "        \n",
    "    for i in range(int(len(also_viewed)*0.9),len(also_viewed)):\n",
    "        if (iid,also_viewed[i]) in train_pair_set:\n",
    "            continue\n",
    "        test_pairs.append((iid,also_viewed[i]))\n",
    "        assert (iid,also_viewed[i]) not in train_pair_set\n",
    "        \n",
    "        # add to item_id2idx\n",
    "        if also_viewed[i] not in item_id2idx:\n",
    "            item_id2idx[also_viewed[i]] = len(item_id2idx)\n",
    "        \n",
    "        # add to train_user_neighbor/train_item_neighbor\n",
    "        test_neighbor[iid].append(also_viewed[i])\n",
    "        \n",
    "print(f'Train/Val/Test size:{len(train_pairs)},{len(val_pairs)},{len(test_pairs)}')\n",
    "print(f'Train/Val/Test avg:{len(train_pairs)/len(ref_paper)},{len(val_pairs)/len(ref_paper)},{len(test_pairs)/len(ref_paper)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea49f922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47099/47099 [00:00<00:00, 100380.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# save all the text on node in the graph\n",
    "\n",
    "node_id_set = set()\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/corpus.txt','w') as fout:    \n",
    "    for iid in tqdm(ref_paper):\n",
    "        also_viewed = ref_paper[iid]['reference']\n",
    "        \n",
    "        # save iid text\n",
    "        if iid not in node_id_set:\n",
    "            node_id_set.add(iid)\n",
    "            fout.write(iid+'\\t'+text_process(data[iid]['title'])+'\\n')\n",
    "    \n",
    "        # save neighbor\n",
    "        for iid_n in also_viewed:\n",
    "            if iid_n not in node_id_set:\n",
    "                node_id_set.add(iid_n)\n",
    "                fout.write(iid_n+'\\t'+text_process(data[iid_n]['title'])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ed2498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_neighbor_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e668bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 191151/191151 [00:12<00:00, 15449.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save train file\n",
    "\n",
    "random.seed(0)\n",
    "sample_neighbor_num = 5\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/train.text.jsonl','w') as fout:\n",
    "    for (q, k) in tqdm(train_pairs):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        random.shuffle(k_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num-len(k_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        q_text = text_process(data[q]['title'])\n",
    "        #q_n_text = '\\*\\*'.join([text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples])\n",
    "        q_n_text = [text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples]\n",
    "        \n",
    "        k_text = text_process(data[k]['title'])\n",
    "        #k_n_text = '\\*\\*'.join([text_process(data[k_n]['title']) if k_n != -1 else '' for k_n in k_samples])\n",
    "        k_n_text = [text_process(data[k_n]['title']) if k_n != -1 else '' for k_n in k_samples]\n",
    "        \n",
    "        #q_line = q_text + '\\t' + q_n_text\n",
    "        #k_line = k_text + '\\t' + k_n_text\n",
    "        \n",
    "        #fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'k_text':k_text,\n",
    "            'k_n_text':k_n_text,\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1af48f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20836/20836 [00:01<00:00, 11789.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save val file (make sure to delete items that are not in train set)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/val.text.jsonl','w') as fout:\n",
    "    for (q, k) in tqdm(val_pairs):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        random.shuffle(k_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num-len(k_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        q_text = text_process(data[q]['title'])\n",
    "        #q_n_text = '\\*\\*'.join([text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples])\n",
    "        q_n_text = [text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples]\n",
    "        \n",
    "        k_text = text_process(data[k]['title'])\n",
    "        #k_n_text = '\\*\\*'.join([text_process(data[k_n]['title']) if k_n != -1 else '' for k_n in k_samples])\n",
    "        k_n_text = [text_process(data[k_n]['title']) if k_n != -1 else '' for k_n in k_samples]\n",
    "        \n",
    "        #q_line = q_text + '\\t' + q_n_text\n",
    "        #k_line = k_text + '\\t' + k_n_text\n",
    "        \n",
    "        #fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'k_text':k_text,\n",
    "            'k_n_text':k_n_text,\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b14bc30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55414/55414 [00:03<00:00, 15958.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save test file (make sure to delete items that are not in train set)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/test.text.jsonl','w') as fout:\n",
    "    for (q, k) in tqdm(test_pairs):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        random.shuffle(k_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num-len(k_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        q_text = text_process(data[q]['title'])\n",
    "        #q_n_text = '\\*\\*'.join([text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples])\n",
    "        q_n_text = [text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples]\n",
    "        \n",
    "        k_text = text_process(data[k]['title'])\n",
    "        #k_n_text = '\\*\\*'.join([text_process(data[k_n]['title']) if k_n != -1 else '' for k_n in k_samples])\n",
    "        k_n_text = [text_process(data[k_n]['title']) if k_n != -1 else '' for k_n in k_samples]\n",
    "        \n",
    "        #q_line = q_text + '\\t' + q_n_text\n",
    "        #k_line = k_text + '\\t' + k_n_text\n",
    "        \n",
    "        #fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'k_text':k_text,\n",
    "            'k_n_text':k_n_text,\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22662de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save side files\n",
    "pickle.dump([sample_neighbor_num],open(f'data_dir/{dataset}/{sub_dataset}/neighbor_sampling.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2092165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save neighbor file\n",
    "pickle.dump(train_neighbor,open(f'data_dir/{dataset}/{sub_dataset}/neighbor/train_neighbor.pkl','wb'))\n",
    "pickle.dump(val_neighbor,open(f'data_dir/{dataset}/{sub_dataset}/neighbor/val_neighbor.pkl','wb'))\n",
    "pickle.dump(test_neighbor,open(f'data_dir/{dataset}/{sub_dataset}/neighbor/test_neighbor.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a294112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47099/47099 [00:01<00:00, 23675.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# save node labels\n",
    "random.seed(0)\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/nc/node_classification.jsonl','w') as fout:\n",
    "    for q in tqdm(ref_paper):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        q_text = text_process(data[q]['title'])\n",
    "        #q_n_text = '\\*\\*'.join([text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples])\n",
    "        q_n_text = [text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples]\n",
    "        \n",
    "        label_names_list = list(set([label_name_dict[lid] for lid in ref_paper[q]['label']]))\n",
    "        label_ids_list = [label_name2id_dict[lname] for lname in label_names_list]\n",
    "        \n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'labels':label_ids_list,\n",
    "            'label_names':label_names_list\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf781d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58186/58186 [00:00<00:00, 1124803.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58186/58186 [00:00<00:00, 84601.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11637/11637 [00:00<00:00, 134216.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11638/11638 [00:00<00:00, 132842.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate self constrastive pretraining\n",
    "\n",
    "corpus_list = []\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/corpus.txt') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = line.strip().split('\\t')\n",
    "        corpus_list.append(tmp[1])\n",
    "        \n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/self-train/train.text.jsonl','w') as fout:\n",
    "    for dd in tqdm(corpus_list):\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':dd,\n",
    "            'q_n_text':[''],\n",
    "            'k_text':dd,\n",
    "            'k_n_text':[''],\n",
    "        })+'\\n')\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/self-train/val.text.jsonl','w') as fout:\n",
    "    for dd in tqdm(corpus_list[:int(0.2*len(corpus_list))]):\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':dd,\n",
    "            'q_n_text':[''],\n",
    "            'k_text':dd,\n",
    "            'k_n_text':[''],\n",
    "        })+'\\n')\n",
    "        \n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/self-train/test.text.jsonl','w') as fout:\n",
    "    for dd in tqdm(corpus_list[int(0.8*len(corpus_list)):]):\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':dd,\n",
    "            'q_n_text':[''],\n",
    "            'k_text':dd,\n",
    "            'k_n_text':[''],\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d3391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "637d3f4b",
   "metadata": {},
   "source": [
    "## Generate node classification data for retrieval and reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc9f4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write labels into documents.json\n",
    "\n",
    "labels_dict = []\n",
    "#for lid in label_name_dict:\n",
    "for lname in label_name2id_dict:\n",
    "    if lname != 'null':\n",
    "        labels_dict.append({'id':label_name2id_dict[lname], 'contents':lname})\n",
    "json.dump(labels_dict, open(f'data_dir/MAG/{sub_dataset}/nc/documents.json', 'w'), indent=4)\n",
    "\n",
    "with open(f'data_dir/MAG/{sub_dataset}/nc/documents.txt', 'w') as fout:\n",
    "    #for lid in label_name_dict:\n",
    "    for lname in label_name2id_dict:\n",
    "        if lname == 'null':\n",
    "            continue\n",
    "        fout.write(label_name2id_dict[lname]+'\\t'+lname+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc2bb387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47099/47099 [00:00<00:00, 86122.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate node query file & ground truth file\n",
    "\n",
    "docid = 0\n",
    "\n",
    "with open(f'data_dir/MAG/{sub_dataset}/nc/node_classification.jsonl') as f, open(f'data_dir/MAG/{sub_dataset}/nc/node_text.tsv', 'w') as fout1, open(f'data_dir/MAG/{sub_dataset}/nc/truth.trec', 'w') as fout2:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = json.loads(line)\n",
    "        fout1.write(str(docid) + '\\t' + tmp['q_text'] + '\\n')\n",
    "        for label in tmp['labels']:\n",
    "            fout2.write(str(docid)+' '+str(0)+' '+label+' '+str(1)+'\\n')\n",
    "        docid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bf4fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37679/37679 [00:01<00:00, 27108.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4710/4710 [00:00<00:00, 33668.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4710/4710 [00:00<00:00, 48783.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate node query file & ground truth file\n",
    "\n",
    "docid = 0\n",
    "\n",
    "with open(f'data_dir/MAG/{sub_dataset}/nc/node_classification.jsonl') as f, open(f'data_dir/MAG/{sub_dataset}/nc/train.text.jsonl', 'w') as fout1, open(f'data_dir/MAG/{sub_dataset}/nc/val.text.jsonl', 'w') as fout2, open(f'data_dir/MAG/{sub_dataset}/nc/test.truth.trec', 'w') as fout3, open(f'data_dir/MAG/{sub_dataset}/nc/test.node.text.jsonl', 'w') as fout4:\n",
    "    readin = f.readlines()\n",
    "    total_len = len(readin)\n",
    "    for line in tqdm(readin[:int(0.8*total_len)]):\n",
    "        tmp = json.loads(line)\n",
    "        for label_name in tmp['label_names']:\n",
    "            fout1.write(json.dumps({\n",
    "                'q_text':tmp['q_text'],\n",
    "                'q_n_text':tmp['q_n_text'],\n",
    "                'k_text':label_name,\n",
    "                'k_n_text':[''],\n",
    "            })+'\\n')\n",
    "        docid += 1\n",
    "    \n",
    "    for line in tqdm(readin[int(0.8*total_len):int(0.9*total_len)]):\n",
    "        tmp = json.loads(line)\n",
    "        for label_name in tmp['label_names']:\n",
    "            fout2.write(json.dumps({\n",
    "                'q_text':tmp['q_text'],\n",
    "                'q_n_text':tmp['q_n_text'],\n",
    "                'k_text':label_name,\n",
    "                'k_n_text':[''],\n",
    "            })+'\\n')\n",
    "        docid += 1\n",
    "        \n",
    "    for line in tqdm(readin[int(0.9*total_len):]):\n",
    "        tmp = json.loads(line)\n",
    "        #fout4.write(str(docid) + '\\t' + tmp['q_text'] + '\\n')\n",
    "        fout4.write(json.dumps({\n",
    "                'id': str(docid),\n",
    "                'text':tmp['q_text'],\n",
    "                'n_text':tmp['q_n_text']\n",
    "            })+'\\n')\n",
    "        for label in tmp['labels']:\n",
    "            fout3.write(str(docid)+' '+str(0)+' '+label+' '+str(1)+'\\n')\n",
    "        docid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783871ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0964a49a",
   "metadata": {},
   "source": [
    "## Generate Coarse-grained Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1b97b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5205/5205 [00:00<00:00, 1049735.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique labels:40;{'40700': 'industrial organization', '50522688': 'economic growth', '187736073': 'management', '138921699': 'political economy', '134560507': 'environmental economics', '26271046': 'economic geography', '21547014': 'operations management', '45355965': 'socioeconomics', '106159729': 'financial economics', '47768531': 'development economics', '121955636': 'accounting', '18547055': 'international economics', '149782125': 'econometrics', '34447519': 'market economy', '145236788': 'labour economics', '556758197': 'monetary economics', '105639569': 'economic policy', '48824518': 'agricultural economics', '139719470': 'macroeconomics', '539667460': 'management science', '167562979': 'classical economics', '133425853': 'neoclassical economics', '175444787': 'microeconomics', '107826830': 'environmental resource management', '10138342': 'finance', '6303427': 'economic history', '4249254': 'demographic economics', '73283319': 'financial system', '144237770': 'mathematical economics', '190253527': 'law and economics', '165556158': 'keynesian economics', '100001284': 'public economics', '162118730': 'actuarial science', '54750564': 'commerce', '118084267': 'positive economics', '136264566': 'economy', '549774020': 'welfare economics', '74363100': 'economic system', '175605778': 'natural resource economics', '155202549': 'international trade'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read label name dict\n",
    "coarse_label_id2name = {}\n",
    "#coarse_label_id2idx = {}\n",
    "\n",
    "with open(f'data_dir/MAG/{sub_dataset}/labels.txt') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = line.strip().split('\\t')\n",
    "        if tmp[2] == '1':\n",
    "            coarse_label_id2name[tmp[0]] = tmp[1]\n",
    "            #coarse_label_id2idx[tmp[0]] = len(coarse_label_id2idx)\n",
    "\n",
    "print(f'Num of unique labels:{len(coarse_label_id2name)};{coarse_label_id2name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742dc636",
   "metadata": {},
   "source": [
    "### Take care here, you need to generate data for 8 & 16 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cdf86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 145761/145761 [00:02<00:00, 52456.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique labels:40;{'144237770': 0, '145236788': 1, '10138342': 2, '149782125': 3, '139719470': 4, '175444787': 5, '50522688': 6, '106159729': 7, '100001284': 8, '190253527': 9, '136264566': 10, '556758197': 11, '21547014': 12, '162118730': 13, '40700': 14, '138921699': 15, '54750564': 16, '45355965': 17, '73283319': 18, '121955636': 19, '47768531': 20, '155202549': 21, '165556158': 22, '118084267': 23, '48824518': 24, '18547055': 25, '4249254': 26, '133425853': 27, '175605778': 28, '26271046': 29, '105639569': 30, '34447519': 31, '134560507': 32, '167562979': 33, '539667460': 34, '187736073': 35, '549774020': 36, '74363100': 37, '107826830': 38, '6303427': 39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate train/val/test file\n",
    "# filter out and only use node which has single label\n",
    "\n",
    "ktrain = 8 # train sample threshold, how many training samples do we have for each class\n",
    "kdev = 8 # dev sample threshold, how many dev samples do we have for each class\n",
    "label_samples = defaultdict(list)\n",
    "\n",
    "with open(f'data_dir/MAG/{sub_dataset}/nc/node_classification.jsonl') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = json.loads(line)\n",
    "        inter_label = list(set(tmp['labels']) & set(coarse_label_id2name))\n",
    "        if len(inter_label) == 1:\n",
    "            label_samples[inter_label[0]].append(tmp)\n",
    "            \n",
    "# select labels\n",
    "coarse_label_id2idx = {}\n",
    "for l in label_samples:\n",
    "    if len(label_samples[l]) > ktrain + kdev:\n",
    "        coarse_label_id2idx[l] = len(coarse_label_id2idx)\n",
    "        \n",
    "print(f'Num of unique labels:{len(coarse_label_id2idx)};{coarse_label_id2idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "747fadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "if not os.path.exists(f'data_dir/MAG/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}'):\n",
    "    os.mkdir(f'data_dir/MAG/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}')\n",
    "\n",
    "with open(f'data_dir/MAG/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}/train.text.jsonl', 'w') as fout1, open(f'data_dir/MAG/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}/val.text.jsonl', 'w') as fout2, open(f'data_dir/MAG/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}/test.text.jsonl', 'w') as fout3:\n",
    "    \n",
    "    assert ktrain+kdev <= 32\n",
    "    \n",
    "    for l in coarse_label_id2idx:\n",
    "        train_data = label_samples[l][:ktrain]\n",
    "        dev_data = label_samples[l][ktrain:(ktrain+kdev)]\n",
    "        #test_data = label_samples[l][(ktrain+kdev):]\n",
    "        test_data = label_samples[l][32:]\n",
    "    \n",
    "        # write train\n",
    "        for d in train_data:\n",
    "            fout1.write(json.dumps({\n",
    "                'q_text':d['q_text'],\n",
    "                'q_n_text':d['q_n_text'],\n",
    "                'label':coarse_label_id2idx[l]\n",
    "            })+'\\n')\n",
    "    \n",
    "        # write dev\n",
    "        for d in dev_data:\n",
    "            fout2.write(json.dumps({\n",
    "                'q_text':d['q_text'],\n",
    "                'q_n_text':d['q_n_text'],\n",
    "                'label':coarse_label_id2idx[l]\n",
    "            })+'\\n')\n",
    "    \n",
    "        # write test\n",
    "        for d in test_data:\n",
    "            fout3.write(json.dumps({\n",
    "                'q_text':d['q_text'],\n",
    "                'q_n_text':d['q_n_text'],\n",
    "                'label':coarse_label_id2idx[l]\n",
    "            })+'\\n')\n",
    "\n",
    "pickle.dump(coarse_label_id2idx, open(f'data_dir/MAG/{sub_dataset}/nc-coarse/coarse_label_id2idx.pkl', 'wb'))\n",
    "pickle.dump([ktrain, kdev], open(f'data_dir/MAG/{sub_dataset}/nc-coarse/threshold.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc996ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227ed55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
